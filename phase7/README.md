# Phase 7: Distributed Tracing & Log Aggrigation with Spring Cloud Sleuth, Zipkin, and RabbitMQ
As microservices scale out, it becomes increasingly important to trace transactions.  This is helpful when we need to debug so that we can discover what went wrong, adn where.  Additionally, if we are utilizing logging in our microservices, looking at all of the logs generated by all services can become a nightmare - that's where log aggregation comes in to help us organize and group our logs generated by separate services.

<br>

Distributed Tracing and Log Aggregation answers these three questions:

#### *How do we debug a problem in microservices?*
We must implement distributed tracing to trace one or more transactions across multiple services, physical machines, and different data stores to try and find exactly where a bug is occuring.  There is a way to append the trace id with the help of a distributed tracing tool, rather than manually logging it each time, which is cumbersome.

<br>

#### *How do we aggregate all application logs?*
We must combine all the logs from multiple services into a central location where they can be indexed, searched, filtered, and grouped to find bugs that are contributing to a problem.

<br>

#### *How do we monitor our chain of service calls?*
We must understand the specific path that a service call took as it travelled within our microservices network, and the time it took as each microservice.  This helps us understand why a requess it taking extra long.

<br>

### Spring Cloud Slueth
- Spring Cloud Sleuth provides Spring Boot auto-configuration for distributed tracing.
- It adds trace and span ids to all the logs, so you can just extract from a given trace or span in a log aggregator.
- It does this by adding filters and interacting with other Spring components to pass the generated correlation ids through all system calls.

> Spring Cloud Sleuth will add three pieces of information to all the logs written by a microservice. <br>
> `[<App Name>, <Trace ID>, <Span ID>` <br>
> `<App Name>`: The application name where the log entry is generated.  This comes from the `spring.application.name` property. <br>
> `<Trace ID>`: Trace ID is the equivelent term for the correlation ID.  It's a unique number that represents the entire transaction. <br> 
> `<Span ID>`: Span ID is a unique ID tha trepresents *part* of the overall transaction,  Each service participating within the transaction will have its own span ID. Span ID's are particularly relevant when you integrat with Zipkin to visualize your transactions.


<br>

### Zipkin
- Zipkin is an open-source data-visualization tool that helps aggregate all the logs and gather timing data needed to troubleshoot latency problems in microservices architecture.
- It allows us to break a transaction down into into components and visually indentify where there might be performance hotspots, thus reducing triage time by contextualizing errors and delays.

<br>

## Implement Distributed Tracing with Spring Cloud Sleuth

1. In*each* of your microservices, add the Spring Cloud Sleuth starter dependency.  It should look like this in your `pom.xml`'s: 

<br>

```xml
<dependency>
    <groupId>org.springframework.cloud</groupId>
    <artifactId>spring-cloud-starter-sleuth</artifactId>
</dependency>
```

<br>

2. We have a few `log.info()` statements inside of `accounts`, `cards`, and `loans`.  So when we send a POST request to the `myCustomerDetails()` method in the `accounts` services, we should generate a trace id that travels through all 3 services.

3. Start config > eureka > accounts > cards > loans > gateway. Clear all of the consoles. (Alternatively to save your machine and some time, make sure all of your images are pushed and run `docker compose up -d` within `accounts/docker-compose/default`)  Then send a POST request to `accounts` with `{ "customerId" : 1 }` as the Request Body at the location `http://localhost:8072/bank/accounts/myCustomerDetails`

4. After you've sent the request, go the the console of any of your services, like `cards` for example.  (*If you're using docker, run `docker ps` to find the contianer id of your `cards` servce > run `docker logs -f <cards-container id>` to follow the logs within the `cards` container*.  This is the first place that receives the request. 

The logs, with the help of Spring Cloud Sleuth, will look like this:

<br>

```sh
2021-10-14 12:10:41.501  INFO [cards,10790a3bbef07309,70dfeef6153bfee7] 1 --- [nio-9000-exec-1] c.r.cards.controller.CardsController     : getCardDetails() method started
2021-10-14 12:10:42.046  INFO [cards,10790a3bbef07309,70dfeef6153bfee7] 1 --- [nio-9000-exec-1] c.r.cards.controller.CardsController     : getCardDetails() method ended
```

<br>

## Implementing Zipkin for Log Aggregation

1. Set up Zipkin Server with docker on port 9411. Run: `docker run -d -p 9411:9411 openzipkin/zipkin` 

2. Visit the UI provided by Zipkin server by going to `http://localhost:9411/zipkin/` in your browser. There are no traces featured in the console because we have to configure our services! Let's do that.

3. Add the Zipkin dependency to each of your services' `pom.xml`. It should look like this:

<br>

```xml
<dependency>
    <groupId>org.springframework.cloud</groupId>
    <artifactId>spring-cloud-sleuth-zipkin</artifactId>
</dependency>
```

<br>

4. Now we have to configure the zipkin endpoint for each service within the service's respective `application.properties` file.